---
title: "HW2_harinris"
author: "Harin Rishabh"
date: "2023-09-27"
output: html_document
---

# Lab 3.6

## 3.6.1 Libraries

```{r}
library(MASS)
library(ISLR2)
```

## 3.6.2 Simple Linear Regression

Using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor.
```{r}
head(Boston)
attach(Boston)
lm.fit <- lm(medv ~ lstat)
summary(lm.fit)
```

Accessing information within lm.fit
```{r}
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
```

The predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat
```{r}
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
```

Plotting medv and lstat along with the least squares regression line using the plot() and abline() functions
```{r}
plot(lstat, medv, pch = "+")
abline(lm.fit, lwd = 3, col = "red")
plot (1:20, 1:20, pch = 1:20)
par(mfrow = c(2 , 2))
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot (hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

## 3.6.3 Multiple Linear Regression

The syntax lm(y ∼ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3
```{r}
lm.fit = lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

Using all 12 variables as predictors
```{r}
lm.fit = lm(medv ~ ., data = Boston)
summary(lm.fit)
```

We want to remove age as a predictor due to its high p-value.
```{r}
lm.fit1 <- update(lm.fit, ~ . - age)
```

## 3.6.4 Interaction Terms

lstat * age includes lstat, age, and the interaction term lstat × age as predictors
```{r}
summary(lm(medv ~ lstat * age, data = Boston))
```

## 3.6.5 Non-linear Transformations of the Predictors

Performing a regression of medv onto lstat and lstat^2
```{r}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
```

Using anova() to see how much better our new quadratic fit is compared to the linear fit
```{r}
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
par(mfrow = c(2 , 2))
plot(lm.fit2)
```

Trying a fifth order polynomial transformation
```{r}
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
```

Trying a log transformation
```{r}
summary(lm(medv ~ log(rm), data = Boston))
```

## 3.3.6 Qualitative Predictors
Given a qualitative variable such as Shelveloc, R generates dummy variables automatically. contrasts()

```{r}
lm.fit <- lm(Sales ~ . + Income : Advertising + Price : Age, data = Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
```

## 3.6.7 Writing Functions

Writing and calling functions in R.
```{r}
LoadLibraries <- function() {
library(ISLR2)
library(MASS)
print("The libraries have been loaded.")
}
LoadLibraries()
```


# Exercise 3.7

**8) a)**\
Answer - 
```{r}
attach(Auto)
lm.fit = lm(mpg ~ horsepower)
summary(lm.fit)
```
**8) a) i)**\
Answer - As value p < 2.2e-16, there is a relationship between the predictor and the response which is statistically significant.
```{r}
lm.fit<-lm(mpg~horsepower,data=Auto)
summary(lm.fit)
```

**8) a) ii)**\
Answer - 60.59% of the variance in mpg is explained by horsepower. 
```{r}
summary(lm.fit)$r.squared
summary(lm.fit)$sigma
summary(lm.fit)$sigma/mean(Auto$mpg)
```

**8) a) iii)**\
Answer - Negative
```{r}
coefficients(lm.fit)["horsepower"]
```

**8) a) iv)**\
Answer - Prediction interval = 24.467 ± 9.658
```{r}
predict(lm.fit, data.frame(horsepower = 98), interval = "confidence", level = 0.95)
```

**8) b)**\
Answer - 
```{r}
plot(horsepower, mpg)
abline(lm.fit, lwd=3, col="red")
```

**8) c)**\
Answer -
```{r}
par(mfrow = c(2 , 2))
plot(lm.fit)
```
**9) a)**\
Answer - 
```{r}
pairs(Auto, pch = 19)
```

**9) b)**\
Answer -
```{r}
Auto$name = NULL
cor(Auto, method="pearson")
```

**9) c) i)**\
Answer - Yes, there is a relation between the predictors and response. According to the null hypothesis all the coefficients are 0. F-statistic is used to test the hypothesis. As p-value < 2.2e-16, null hypothesis is rejected.
```{r}
lm.fit<-lm(mpg~. , data=Auto)
summary(lm.fit)
```


**9) c) ii)**\
Answer - Displacement, weight, year, origin

**9) c) iii)**\
Answer - mpg value increases when every other predictor is kept constant. If there is an increase of a unit in year then there will be increase of 0.7507727 unit in mpg.
```{r}
coef(lm.fit)["year"]
```


**9) d)**\
Answer - The first graph shows that there is a non-linear relationship between the response and the predictors. The second graph shows that the residuals are normally distributed and right skewed. The third graph shows that the constant variance of error assumption is False. The fourth graphs shows that there are no leverage points. However, there's one potential leverage point.
```{r}
par(mfrow = c(2,2))
plot(lm.fit,col = "red")
```

**9) e)**\
Answer - Yes. few interactions are statistically significant as shown by the Signif. codes/p-values in the summary below For example, acceleration and origin
```{r}
lm.fit = lm(mpg ~ .*. + displacement:weight,data=Auto)
summary(lm.fit)
```

**9) f)**\
Answer - Among these plots, log(horsepower) seems to be most linear looking.
```{r}
par(mfrow = c(2, 2))
plot(log(Auto$horsepower), Auto$mpg)
plot(sqrt(Auto$horsepower), Auto$mpg)
plot((Auto$horsepower)^2, Auto$mpg)
```

**13) a)**\
Answer - 
```{r}
set.seed(1)
X = rnorm(100, mean = 0, sd = 1)
```

**13) b)**\
Answer - 
```{r}
eps = rnorm(100, mean = 0, sd = sqrt(0.25))
```

**13) c)**\
Answer - Lenght is 100. Beta 0 is -1 and Beta 1 is 0.5.
```{r}
Y = -1 + 0.5 * X + eps
length(Y)
```

**13) d)**\
Answer - X and Y have a positive correlation and they have a linear relationship
```{r}
plot(X, Y,col="red")
```

**13) e)**\
Answer - The estimated values are very close, the presence of irreducible error prevents a perfect fit.
```{r}
model1 = lm(Y ~ X)
coef(model1)
summary(model1)$adj.r.squared
```

**13) f)**\
Answer -
```{r}
plot(X, Y, typ = "p")
abline(model1, col = "red", lty = "dotted")
abline(a = -1, b = 0.5, col = "blue", lty = "solid")
legend("bottomright", legend = c("actual", "estimate"), col = c("blue", "red"), lty = c("solid", "dotted"))
```

**13) g)**\
Answer - No,  2 coefficient is not statistically significant. i.e.Pr(>|t|) is 1.638275e-01 (which is greater than 0.05, a common rejection threshold). As the Adjusted R squared estimate is slightly larger which implies this is a better model.
```{r}
model2 = lm(Y ~ poly(X, 2))
summary(model2)$coefficients
summary(model2)$adj.r.squared
```

**13) h)**\
Answer - The model is more accurate as noise has decreased.
```{r}
eps3 = rnorm(100, mean = 0, sd = 0.001)
y3 = -1 + 0.5 * X + eps3
model3 = lm(y3 ~ X)
coef(model3)
summary(model3)$adj.r.squared
plot(X, y3, typ = "p")
abline(model3, col = "red", lty = "dotted")
abline(a = -1, b = 0.5, col = "blue", lty = "solid")
legend("bottomright", legend = c("actual", "estimate"), col = c("blue", "red"), lty = c("solid", "dotted"))
```
**13) i)**\
Answer - The model is less accurate as noise has increased.
```{r}
eps4 = rnorm(100, mean = 0, sd = 50)
y4 = -1 + 0.5 * X + eps4
model4 = lm(y4 ~ X)
coef(model4)
summary(model4)$adj.r.squared
plot(X, y4, typ = "p")
abline(model4, col = "red", lty = "dotted")
abline(a = -1, b = 0.5, col = "blue", lty = "solid")
legend("bottomright", legend = c("actual", "estimate"), col = c("blue", "red"), lty = c("solid", "dotted"))
```

**13) i)**\
Answer - Noisier data produces larger confidence intervals for the coefficients.
```{r}
confint(model1)
confint(model3)
confint(model4)
```

**14) a)**\
Answer = y = 2 + 2x1 + 0.3x2 where coefficients are 2, 2, 0.3
```{r}
set.seed(1)
x1 = runif(100)
x2 = 0.5*x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

**14) b)**\
Answer =
```{r}
cor(x1, x2)
plot(x1, x2)
```

**14) c)**\
Answer = The coefficients are 2.131, 1.44, 1.01.
Checking with the p-values of our parameters, we cannot reject the hypothesis for beta 1 or beta 2 with 95% confidence. The results indicate nontrivial bias between all of the parameters.
```{r}
model = lm(y ~ x1 + x2)
summary(model)
```

**14) d)**\
Answer = We can confidently reject the null hypothesis now.
```{r}
model = lm(y ~ x1)
summary(model)
```

**14) e)**\
Answer = We can confidently reject the null hypothesis now.
```{r}
model = lm(y ~ x2)
summary(model)
```
**14) f)**\
Answer - The results do not contradict each other as the predictors are highly correlated and are collinear. Due to collinearity its difficult to determine how each predictor is associated with the response.

**14) g)**\
Answer - In the model with two predictors, the last point is a high-leverage point. In the model with x1 as sole predictor, the last point is an outlier. In the model with x2 as sole predictor, the last point is a high leverage point.
```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
fit1 <- lm(y ~ x1 + x2)
fit2 <- lm(y ~ x1)
fit3 <- lm(y ~ x2)
summary(fit1)
summary(fit2)
summary(fit3)
plot(fit1)
plot(fit2)
plot(fit3)
```
